# KDCGN

**Tensorflow** implementation of 'Knowledge Distillation Classifier Generation Network for Zero-Shot Learning'

![image](https://github.com/yunlongyu/KDCGN/tree/main/imgs/fig1.png)

## Preparation

- **Prerequisites**
    - Tensorflow (r1.4 - r1.12 should work fine)
    - Python 2.7 or 3.5 with matplotlib, numpy and scipy
    
- **Datasets**
   - Original Data: download the data from [Original Data](https://drive.google.com/file/d/1H-lYL4Ol2D1HIX5jrJvfx5zy7SmGANsu/view?usp=sharing);
   - Auxiliary Data: the auxiliary classes for AwA datasets are provided [Auxiliary Data](https://drive.google.com/file/d/1RD6GnSZVFeyJ2p6k0Vz8MYS92hItxLLI/view?usp=sharing)
   - Put the download data in the corresponding files.
   
